\chapter{Convexity and optimization}
\thispagestyle{empty}  % Remove opening page number

	Catenaries, Fermat's principle of least time and thermodynamic equilibrium are well known examples of extremum principles applied to the description of nature. On the opposite side, multiple essential modern technologies, such as network routing, logistics and integrated circuit design rely on finding good solutions to optimization problems. No matter whether you are minimizing a Gibbs free energy under constant $T$ and $P$, or searching for the best arrangement of electronic components on a chip to reduce its footprint while satisfying heat dissipation and fabrication constraints, this is what an optimization problem looks like
	%
	\begin{equation}
		p^* = \max \big\{ f_0(x) \mid f_i(x) \leq b_i, \,\forall i \in \{1, \ldots, m\} \big\} .
		\label{eq:optimization-program}
	\end{equation}
	%
	Solving it means finding some $x^* \in \mathbb{R}^n$ such that the value of the \emph{objective function} $f_0(x^*) \leq f_0(x), \,\forall x \in \mathcal{F}$, where
	$$
	\mathcal{F} = \big\{ x \mid f_i(x) \leq b_i , \,\forall i \in \{1, \ldots, m\} \big\}
	$$
	is the \emph{feasible set}. The objective function represents the quantity to be minimized (the free energy; the footprint), while $\mathcal{F}$ imposes the constraints (constant temperature and pressure; rate of heat dissipation and limitations of the fabrication procedure).
	
	Finding global optima for nonlinear programs is difficult, and no general methods to do so are known. That is why one usually consider subsets of eq.~\ref{eq:optimization-program} where some special stucture is imposed on the $f_i$. Our objective for this chapter is to learn how to recognize the special structures of linear and semidefinite programs, which requires some groundwork on convexity. Furthermore, polytopes --- a special type of convex sets ---, are ubiquitous in the geometry of correlation scenarios, and will make an appearance in later chapters. This will be our starting point.	


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Convexity}
	\label{sec:convexity}

		This subsection closely follows the exposition in \cite{rockafellar,grumbaum,ziegler,boyd}, where most of the alluded proofs can be found. Unless otherwise specified, all sets are subsets of $\mathbb{R}^d$.

		Let $x_1 \neq x_2$ be two elements of some set.
		$$
			\alpha x_1 + (1 - \alpha) x_2, \quad\alpha \in \mathbb{R}
		$$
		is the \emph{line} passing through $x_1$ and $x_2$. A set $A$ containing all lines passing through its elements is an \emph{affine set}. The real line and the Cartesian plane are affine sets, but a sphere and a cube are not.

		Finite induction shows that, for $x_1, \ldots, x_n \in A$ and $\sum_i \alpha_i = 1$, the point $x = \sum_i \alpha_i x_i \in A$.  Such a sum is an \emph{affine combination} of the $x_i$.

		When $A_i$ are all affine sets, $A = \bigcap_i A_i$ is also affine. Intersections can only decrease cardinality. Hence, if we take some (not necessarily affine) set $S$ and intersect all $A_i \supseteq S$, we will get the smallest affine set containing $S$. This is called an \emph{affine hull}, and denoted $\aff{S}$. Equivalently, it can be shown that
		$$
			\aff{S} = \Big\{ \sum_i \alpha_i x_i \mid \sum_i \alpha_i = 1, \, x_i \in S \Big\} .
		$$

		The affine hull of two points in $\mathbb{R}^n$ is the line through them, and for three noncollinear points we get a plane.

		Vector spaces and affine sets are close siblings: any vector space is an affine set, but the converse is not true, for the latter may not contain the $0$ vector. Furthermore, each affine set $A$ is parallel to a unique vector space $V$. Taking any $x_0 \in A$, we can translate $A - x_0 = V$. The \emph{dimension} of an affine space $A$ is the dimension of its parallel vector space $V$. In $\mathbb{R}^n$, dimensions $0, 1, 2$ and $n-1$ corresponds to points, lines, planes and hyperplanes.

		Any hyperplane $H$ can be represented as the set
		$$
			H = \{ u \mid \braket{u}{b} = \beta \} ,
		$$
		where $b \in V$, $\beta \in \mathbb{R}$ are constants, and $\braket{\cdot}{\cdot}$ is an inner product on $V$. Switching from equality to $<, >, \leq$ or $\geq$, we get either open or closed \emph{halfspaces}. Halfspaces contain halflines, so they are not affine sets. Rather, they are convex sets.

		Convex sets are somewhat similar to affine sets. But, instead of lines, a convex set $C$ must contain all \emph{line segments}
		$$
			\alpha x_1 + (1 - \alpha) x_2, \quad\alpha \in [0,1]
		$$
		passing through any $x_1, x_2 \in C$. Any affine set is trivially convex, but a sphere and a cube also are.

		Convex combinations are affine combinations with the extra condition that the $\alpha_i \geq 0$. Likewise, it can be shown that a set $C$ is convex if and only if it contains all convex combinations of its elements. They are also closed under intersections, and the convex hull of a (not necessarily convex) set $S$ is the smallest convex set containing $S$,
		$$
			\conv{S} \equiv \bigcap \big\{ C \supseteq S \mid C \text{ is convex} \big\} .
		$$
		Equivalently, it is also the set of all convex combinations of $S$'s elements,
		$$
			\conv{S} = \Big\{ \sum_i \alpha_i x_i \mid \alpha_i \geq 0, \sum_i \alpha_i = 1, \, x_i \in S \Big\} .
		$$

		Convex sets can be further divided into extremal and nonextremal points. An $x \in C$ is an \emph{extreme point} of $C$ when it is not in the relative interior of any segment of $C$. Respectively, when $x = \alpha y + (1 - \alpha) z \,\Rightarrow\, x=y=z$ for any $y, z \in C$ and $\alpha \in (0, 1)$.

		\emph{Polyhedra} are intersections of \emph{finitely} many closed halfspaces. Any polyhedron $L \subset \mathbb{R}^d$ can thus be written as
		$$
			L(A, b) = \big\{ x \in \mathbb{R}^d \mid Ax \leq b \big\} ,
		$$
		where $A \in \mathbb{R}^{d \times m}$ and $b \in \mathbb{R}^m$ specify a set of linear inequalities. A single halfspace, which is obviously unbounded, fits the definition. If a polyhedron $L$ is furthermore bounded (i.e., has no rays), it is a convex \emph{polytope}, denoted by $P$. They are also the convex hull of finitely many points. One way to test if $x \in L$ or $x \in P$ is to check whether it does not violate any of the inequalities defining the halfspaces. Polyhedrons and polytopes inherit their dimensions from their affine hull's.

		As much as cubes have vertices, edges and faces, polytopes have $k$-faces. Any \emph{face} $F$ of a polytope $P$ is a set
		$$
			F = P \,\cap\, \{ x \in \mathbb{R}^d \mid c^\intercal x = b \} ,
		$$
		where $c \in \mathbb{R}^d$ is a column-vector, $b \in \mathbb{R}$, and any $x \in P$ is such that $c^\intercal x \leq b$. Visually, a face is any intersection of $P$ with a closed halfspace that touches $P$ but is not inside of it. The dimension $k$ of the affine hull of $F$ goes into the name $k$-face. Faces of dimension $0$ and $1$ are vertices and edges, while faces of dimension $\text{dim} (P) - 1$ are especially named \emph{facets}. For a cube ($\mathbb{R}^3$), the facets would be the common faces.

		Our previous definitions of polyhedra and polytopes are sometimes termed $\mathcal{H}$-polyhedra and $\mathcal{H}$-polytopes. An alternative definition that will later become important is that of $\mathcal{V}$-polytopes, which are the convex hull of a \emph{finite} set of points. After the hull is performed, they become the extremal points of the convex set.

		$\mathcal{H}$ and $\mathcal{V}$ representations are mathematically equivalent (theorem 1.1 in \cite{ziegler}) but, computationally, the choice of description may significantly matter. As will be made explicit in chaps. \ref{chap:pam} and \ref{chap:pam-classicality}, classicality in prepare-and-measure scenarios can be interpreted as the belonging of a behavior in a polytope. Conversely, nonclassicality may be witnessed through the violation of some inequality defining its facets. The $\mathcal{H}$-description of these polytopes is preferable for being much more economical and ergonomic. Much work in other correlation scenarios is also dedicated to finding all inequalities defining the facets of some polytope. In Bell scenarios, for instance, they are the celebrated Bell inequalities. However, no general methods to directly build them are known. On the other hand, it is conceptually easy to enumerate all extremal points of classicality polytopes (the $\mathcal{V}$-description). Casting one to the other can be done through specialized, user-friendly softwares such as PORTA, PANDA, lrs and cdd \cite{} but, unfortunately, this a computationally expensive problem that can only be done for the very simplest cases of interest.

		The remainder of this chapter will be dedicated to defining linear and semidefinite programs. Linear programming happens over polyhedron-shaped feasible sets, so we are now equipped to tackle them.	


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Optimization}

		Solving an arbitrary instance of eq.~\eqref{eq:optimization-problem} is a hard problem, and general methods only exist for special instances of optimization programs. Convex optimization (\texttt{conv}) --- which happens when all the $f_i$ are convex functions --- is one of the largest classes of programs that can be efficiently solved to global optimality. Even though $\texttt{LIN} \subset \texttt{SDP} \subset \texttt{CONE} \subset \texttt{CONV}$, specialized algorithms for subsets of convex optimization, such as conic (\texttt{cone}), semidefinite (\texttt{sdp}) and linear (\texttt{lin}) programming, renders even larger instances of those pratical. Although we will not discuss them, the main reference for convex programming is the textbook \cite{boyd_vanderberghe}, and a good reference for cone programming, tuned to quantum information, is \cite{uola}. All of these are important tools in quantum information.

		Because of their use in the industry, ready-to-use solvers for optimization problems are broadly accessible. I will mention them in due time, but algorithms will not be discussed (see, e.g., \cite{papadimitriou,gartner} for linear programming, \cite{vanderberghepaper,gartnersdp} for SDPs and \cite{boyd} for convex optimization).


		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{Linear programming}
		\label{sec:linear-programming}

			Linear programming happens when all $f_i$ are linear functions. Recalling eq.~\eqref{eq:optimization-program}, you will se that the feasible region $\mathcal{F}$ is thence an intersection of halfspaces --- a polyhedron. Apart from being widely used in the industry to optimize supply chains, workforce allocation and delivery routes, it is also useful to search for probability distributions satifying a set of constraints. The significance of this problem will become clearer in chap. \ref{chap:pam-classical}.

			Being linear, the objective function $f_0 : \mathbb{R}^n \mapsto \mathbb{R}$ can conveniently be written as $c^\intercal x$, where $c = (c_1, \ldots, c_n)$ is a constant vector in $\mathbb{R}^n$ defining the quantity to be maximized. Minimizing $c^\intercal x$ is the same as maximizing $-c^\intercal x$, so the discussion holds both ways. For an \emph{unconstrained} problem, that is all --- in this case, unless $f_0$ is constant, the program would end up being \emph{unbounded}, though. Things get more interesting when $x$ is constrained to be in a subset $\mathcal{F} \subset \mathbb{R}^n$. We can do that by saying $0 \leq x_i \leq 1, \,\forall i$, for instance, and then we would be optimizing $c^\intercal x$ over some hypercube. More generally, we allow the constraints to be of the form $a_i^\intercal x \leq b_i$, where $a_i \in \mathbb{R}^n$ and $b_i \in \mathbb{R}$. There is no need to consider equality conditions separately, as we can just use two inequalities with inverted directions. So we do not have to write all constraints separately, a shorthand notation is to build an $m \times n$ matrix $A = [a_1 \; \ldots \; a_m]^\intercal$ and a vector $b = [b_1 \; \ldots \; b_m]^\intercal$ of bounds so that
			%
			\begin{subequations}
				\begin{alignat}{2}
					&\text{given}    &\quad & c, A, b \\
					&\underset{x}{\text{max.}}   &	  & c^\intercal x \\
					&\text{s.t.}    &      & Ax \leq b .
				\end{alignat}
				\label{eq:lp}
			\end{subequations}
			%
			becomes a general linear program. When $\mathcal{F} = \emptyset$, the program is \emph{unfeasible}, meaning there are no vector $x$ that satisfies the constraints. Requiring that $x_i \geq 1$ and $x_i \leq -1$ would certainly put you in that situation. If $\mathcal{F}$ is not empty, then the program may either have a solution, called $p^*$ and happening at $x^*$, or be unbounded. To understand why the latter may be the case, notice that $\mathcal{F}$ is a polyhedron. As such, it may itself be unbounded. Consider, for instance, $x \in \mathbb{R}^2$ with $\mathcal{F} = \{ 0 \leq x_1 \leq 1, x_2 \geq 0 \}$. If $c = [c_1 \; c_2]$, with $c_2 > 0$, the optimal value $p^*$ would go to infinity. On the other hand, $c_2 \leq 0$ would be alright. When $\mathcal{F}$ is a (nonempty) polytope, there is always some solution to the program. A solution is either unique (only a single $x^*$ leads to $p^*$) or there infinitely many solutions yielding the same $p^*$. To understand why this is so, consider the \emph{fundamental theorem of linear programming}, which states that every feasible, bounded linear program has an optimal solution on a vertex of $\mathcal{F}$. If optimal solutions happen at two different vertices, then any $x$ in the line segment between them results in $p^*$. 

			Linear programs are efficiently solvable in practice. The simplex method, proposed by George Dantzig in 1947 \cite{}, builds upon the fact that optimal solutions occur in vertices. Starting from any basic feasible solution (a corner of $\mathcal{F}$), it smartly hops to neighboring vertices along edges that increase $f_0$. If you end up at a basic feasible solution connected with no edges that increase the objective value, or if you visit an unbounded edge, you are done. A rigorous presentation and complexity analysis of it can be found in \cite{papadimitriou}. Interestingly, the simplex method is \emph{not} of polynomial time complexity. Rather, there are families of linear programs for which it performs poorly. Nevertheless, it is a reliable and efficient method in practice, and is widely used (in several variations) up to this day. There are provably polynomial time algorithms in theory, of which the ellipsoid algorithm, published in 1980, was the first \cite{}. Even so, it is not efficient in practice. The more recent interior-point methods are provably polynomial in theory, and fast in practice \cite{}.

			Due to its wide applicability in the industry, there are many open source and proprietary linear programming solvers available, such as GLPK \cite{}, Gurobi \cite{}, CPLEX \cite{} and Mosek \cite{}. Most provide user-friendly interfaces to widely used programming languages, such as C, C++, Python and MATLAB. To further aid in using them, there are also modeling languages that can be used to specify programs in a high-level format, and that internally converts and dispatches the problem to specific solvers. YALMIP \cite{} and CVX \cite{} are widely used inside MATLAB, while PICOS \cite{} and CVXPY \cite{} are common choices when working in Python.	


		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{Semidefinite programming}
		\label{sec:sdp}

			Semidefinite programming is the optimization of a linear function subject to a linear matrix inequality. Largely generalizing linear programming, it has found numerous applications in statistics, economics, control theory, pattern recognition and machine learning, to mention a few (see sec. 2 of \cite{vanderberghe_boyd_1996} and chap. 1 of \cite{boydbook} for a survey). It is also widely used as a tool in approximation algorithms to graph theoretical problems \cite{gartner}, and polynomial and non-commutative polynomial optimization \cite{lasserre,navascues}; the latter being extensively used in quantum information.

			Before we define them, recall that
			$$
				S^n = \{ X \in \mathbb{R}^{n \times n} \mid X = X^\intercal \}
			$$
			is the set of symmetric matrices, and
			$$
				S_+^n = \{ X \in S^n \mid X \succeq 0 \}
			$$
			the one of positive semidefinite matrices. All eigenvalues of a symmetric ($X = X^\intercal$) matrix are real, and the positive semidefiniteness condition ($X \succeq 0$) further requires they must be nonnegative.

			Building upon the general linear program~\eqref{eq:linear-program}, we write a semidefinite program (SDP) as
			%
			\begin{subequations}
				\begin{alignat}{2}
					&\text{given}    &\quad & c, F_0, \ldots, F_n \\
					&\underset{x}{\text{max.}}   &	  & c^\intercal x \\
					&\text{s.t.}    &      & F_0 + \sum_{i=1}^n x_i F_i \succeq 0 .
				\end{alignat}
				\label{eq:sdp}
			\end{subequations}
			%
			Here, $F(x) \equiv F_0 + \sum_{i=1}^n x_i F_i$ is what we call a \emph{linear matrix inequality}. The $n + 1$ matrices $F_0, \ldots, F_m$ are in $S^m$, and we maintain $x \in \mathbb{R}^n$. A semidefinite program is thus the optimization of a linear function $c^\intercal x$ under the constraint that $F(x)$ is positive semidefinite.

			This may seem like a rather arbitrary definition. To debunk this impression, first notice that if $F(x) \succeq 0$ and $F(x) \succeq 0$, then
			$$
				F[ \alpha x + (1 - \alpha) y ] = \alpha F(x) + (1- \alpha) F(y) \succeq 0
			$$
			for all $0 \leq \alpha \leq 1$. Thus, both the objective function and the constraint are convex, and semidefinite programming is a special case of convex optimization. More than that, $S_+^n$ is a convex cone, meaning it is also a special case of conic optimization. Now it seems too specialized, so we further notice that, if we let $F_0 = \text{diag}(b)$ and $F_i = \text{diag}(a_i)$, we recover linear program~\eqref{eq:linear-program}. Hence, $\texttt{LIN} \subset \texttt{SDP} \subset \texttt{CONE} \subset \texttt{CONV}$.

			A general $F(x)$ may have a block-diagonal section of the form $\text{diag}(Ax + b)$, and a more general structure in the remainder. Thus, a linear matrix inequality represents an affine section of $S_+^n$, also called an \emph{spectrahedron}. Unlike polyhedra, they may have curved boundaries. A cylinder, for instance, can be parametrized as an $F(x)$ with $4 \times 4$ matrices \cite{https://www.ams.org/notices/201405/rnoti-p492.pdf}. A last, interesting link to linear programming is that $F(x) \succeq 0$ if and only if $z^\intercal F(x) z \geq 0$ for all $z \in \mathbb{R}^m$. These are infinitely many linear constraints on $x$.

			SDPs can also be efficiently solved both in theory and in practice. Interior-point methods are usually robust and efficient, and are implemented in some user-friendly solvers such as SDPA \cite{}, SDPT3 \cite{} and Mosek \cite{}. Performance comparisons between these and several other SDP solver are available at \cite{http://plato.asu.edu/ftp/sparse_sdp.html}.

			The usefulness of semidefinite programming in quantum information is hinted by the fact that quantum states and measurement effects are positive semidefinite matrices. As long as the objective function is linear, we can optimize over them, and they have proven to be useful in quantum state discrimination, quantum steering and hierarchies for nonlocal correlations, among other applications. One of them will be shown in chap. \ref{chap:pam-dense-coding}.

			A drawback in dealing with density operators of measurement effects is that, while semidefinite programming is, as in here, usually discussed over the real field, quantum objects make use of complex numbers. One can nevertheless embed them in real variables. This can be a cumbersome task to do manually but, luckily, the modeling interfaces mentioned in the last section can do this under the hood before calling the solver.


			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\subsubsection{Optimizing the measurement incompatibility robustness}
			\label{sec:incompatibility-robustness}
				\todo{}